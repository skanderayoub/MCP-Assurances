{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "6e27d73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "import re\n",
    "import json\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from pypdf import PdfReader\n",
    "import uuid\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tag import pos_tag\n",
    "from math import log\n",
    "import numpy as np\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "933a16f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PdfTextExtractor:\n",
    "    def __init__(self, pdf_path, output_txt_path):\n",
    "        self.pdf_path = pdf_path\n",
    "        self.output_txt_path = output_txt_path\n",
    "\n",
    "    def extract_text(self):\n",
    "        if not os.path.exists(self.output_txt_path):\n",
    "            reader = PdfReader(self.pdf_path)\n",
    "            text = \"\"\n",
    "            for page in reader.pages:\n",
    "                content = page.extract_text()\n",
    "                if content:\n",
    "                    text += content + \"\\n\"\n",
    "            with open(self.output_txt_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(text)\n",
    "        else:\n",
    "            with open(self.output_txt_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                text = f.read()\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "879405c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextCleaner:\n",
    "    @staticmethod\n",
    "    def clean_text(text):\n",
    "        sub_pattern = r'Code des assurances - Dernière modification le 15 août 2025 - Document généré le 14 août 2025'\n",
    "        text = re.sub(sub_pattern, '', text)\n",
    "        return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "d13474ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HierarchyParser:\n",
    "    def __init__(self):\n",
    "        self.patterns = [\n",
    "            r'(Partie législative|Partie réglementaire - Arrêtés|Partie réglementaire)\\n',\n",
    "            r\"(^Livre [IVXLCDM]+.*$)\",\n",
    "            r\"(^Titre [IVXLCDM]+.*$)\",\n",
    "            r\"(^Chapitre [IVXLCDM]+.*$)\",\n",
    "            r\"(^Section (?:[IVXLCDM]+|[0-9]+)+.*$)\",\n",
    "            r\"(^Sous-section\\s+(?:[IVXLCDM]+|[0-9]+).*$)\"\n",
    "        ]\n",
    "        self.level_keys = [\"partie\", \"livre\", \"titre\", \"chapitre\", \"section\", \"sous_section\"]\n",
    "        self.article_pattern = r'(Article\\s+[A-Z]\\*?\\d+(?:-\\d+)*)'\n",
    "\n",
    "    def split_by_articles(self, text):\n",
    "        articles_splits = re.split(self.article_pattern, text, flags=re.M)\n",
    "        return articles_splits[1::2], articles_splits[2::2], articles_splits[0::2]\n",
    "\n",
    "    def detect_hierarchy(self, preceding_text, prev_hierarchy):\n",
    "        curr_hierarchy = prev_hierarchy.copy()\n",
    "        for idx, pattern in enumerate(reversed(self.patterns)):\n",
    "            splt = re.split(pattern, preceding_text, flags=re.M)\n",
    "            if len(splt) > 1:\n",
    "                new_val = f\"{splt[-2].strip()} {splt[-1].strip()}\"\n",
    "                preceding_text = \"\".join(s for s in splt[:-2])\n",
    "                # print(new_val)\n",
    "                curr_idx = len(self.level_keys) - idx - 1\n",
    "                if curr_hierarchy[self.level_keys[curr_idx]] != new_val:\n",
    "                    curr_hierarchy[self.level_keys[curr_idx]] = new_val\n",
    "                    for lower_idx in range(curr_idx + 1, len(self.level_keys)):\n",
    "                        if (curr_hierarchy[self.level_keys[lower_idx]]) == prev_hierarchy[self.level_keys[lower_idx]]:\n",
    "                            curr_hierarchy[self.level_keys[lower_idx]] = \"\"\n",
    "        return curr_hierarchy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "dca9f7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArticleProcessor:\n",
    "    def __init__(self):\n",
    "        self.stop_words = set(stopwords.words('french')) | {'le', 'la', 'les', 'de', 'du', 'des', 'et', 'en', 'pour', 'par'}\n",
    "        self.documents = []  # Store all article contents for TF-IDF\n",
    "        self.word_doc_freq = defaultdict(int)  # Document frequency for words\n",
    "        self.llm = Ollama(\n",
    "            model=\"llama3.2:1b\",\n",
    "            temperature=0.1\n",
    "        )\n",
    "\n",
    "    def extract_keywords(self, content, top_n=5):\n",
    "        # Tokenize and tag parts of speech\n",
    "        words = word_tokenize(content.lower())\n",
    "        tagged_words = pos_tag(words, lang='eng')  # Using English POS tagger as French is less reliable\n",
    "        # Filter for nouns (NN, NNS, NNP, NNPS) and adjectives (JJ, JJR, JJS)\n",
    "        candidates = [word for word, pos in tagged_words if pos in ('NN', 'NNS', 'NNP', 'NNPS', 'JJ', 'JJR', 'JJS') and word not in self.stop_words and len(word) > 2]\n",
    "\n",
    "        # Calculate TF-IDF\n",
    "        word_freq = defaultdict(int)\n",
    "        for word in candidates:\n",
    "            word_freq[word] += 1\n",
    "\n",
    "        tfidf_scores = {}\n",
    "        total_docs = len(self.documents) if self.documents else 1\n",
    "        for word, freq in word_freq.items():\n",
    "            tf = freq / max(len(candidates), 1)\n",
    "            idf = log(total_docs / (self.word_doc_freq[word] + 1)) + 1\n",
    "            tfidf_scores[word] = tf * idf\n",
    "\n",
    "        # Sort and select top keywords\n",
    "        sorted_keywords = sorted(tfidf_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "        return [word for word, score in sorted_keywords[:top_n]]\n",
    "\n",
    "    def generate_summary(self, content):\n",
    "        prompt = (\n",
    "        \"Provide a concise summary of the following French insurance code article in about 100 characters in french. \"\n",
    "        \"Focus on key legal obligations and procedures, using precise legal terminology and only give the summary as answer:\\n\\n\"\n",
    "        f\"{content}\"\n",
    "        )\n",
    "        response = self.llm.complete(prompt)\n",
    "        summary = str(response).strip()\n",
    "        return summary\n",
    "\n",
    "    def process_article(self, article_id, content, curr_hierarchy, reference_graph, all_articles):\n",
    "        content = re.sub(r\"\\n\", \" \", content.strip())\n",
    "        # Update document frequency for TF-IDF\n",
    "        words = set(word_tokenize(content.lower()))\n",
    "        for word in words:\n",
    "            self.word_doc_freq[word] += 1\n",
    "        self.documents.append(content)\n",
    "\n",
    "        references = re.findall(r\"[A-Z]\\.\\s*\\d{3}-\\d+(?:-\\d+)?\", content)\n",
    "        for ref in references:\n",
    "            reference_graph[\"Article \" + re.sub(\". \", \"\", ref)].append(article_id)\n",
    "            \n",
    "        patterns = [\n",
    "            r'Partie législative',\n",
    "            r'Partie réglementaire - Arrêtés',\n",
    "            r'Partie réglementaire',\n",
    "            r'Livre [IVXLCDM]+',\n",
    "            r'Titre [IVXLCDM]+',\n",
    "            r'Chapitre [IVXLCDM]+',\n",
    "            r'Section (?:[IVXLCDM]+|[0-9]+)+',\n",
    "            r'Sous-section\\s+(?:[IVXLCDM]+|[0-9]+)'\n",
    "        ]\n",
    "\n",
    "        for pattern in patterns:\n",
    "            match = re.search(pattern, content)\n",
    "            if match:\n",
    "                # Cut everything from this match onward\n",
    "                content = content[:match.start()]\n",
    "                break  # Stop after the first match\n",
    "\n",
    "        article = {\n",
    "            \"article_id\": article_id,\n",
    "            \"content\": content.strip(),\n",
    "            \"hierarchy\": curr_hierarchy.copy(),\n",
    "            \"references\": [\"Article \" + re.sub(\". \", \"\", r) for r in references],\n",
    "            \"referenced_by\": [],\n",
    "            \"summary\": \"\", # self.generate_summary(content),\n",
    "            \"keywords\": self.extract_keywords(content),\n",
    "            \"page_number\": None\n",
    "        }\n",
    "        all_articles.add(article_id)\n",
    "        return article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "68d7e467",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CodeAssurancesProcessor:\n",
    "    def __init__(self, pdf_path=\"LEGITEXT000006073984.pdf\", txt_path=\"code_assurances_raw.txt\", json_path=\"code_assurances.json\"):\n",
    "        self.pdf_path = pdf_path\n",
    "        self.txt_path = txt_path\n",
    "        self.json_path = json_path\n",
    "        self.extractor = PdfTextExtractor(pdf_path, txt_path)\n",
    "        self.cleaner = TextCleaner()\n",
    "        self.parser = HierarchyParser()\n",
    "        self.article_processor = ArticleProcessor()\n",
    "        self.hierarchy_tree = defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: defaultdict(dict)))))\n",
    "        self.articles_list = []\n",
    "        self.reference_graph = defaultdict(list)\n",
    "        self.all_articles = set()\n",
    "\n",
    "    def process(self):\n",
    "        # Extract and clean text\n",
    "        text = self.extractor.extract_text()\n",
    "        text = self.cleaner.clean_text(text)\n",
    "        \n",
    "        # Split text by articles\n",
    "        articles_id, articles_content, preceding_texts = self.parser.split_by_articles(text)\n",
    "        print(\"Number of articles:\", len(articles_id))\n",
    "\n",
    "        # Process articles\n",
    "        prev_hierarchy = {lvl: \"\" for lvl in self.parser.level_keys}\n",
    "        for i, article_id in tqdm(enumerate(articles_id[:2]), desc=\"Processing articles\"):\n",
    "            preceding_text = preceding_texts[i] if i < len(preceding_texts) else \"\"\n",
    "            print(article_id)\n",
    "            curr_hierarchy = self.parser.detect_hierarchy(preceding_text, prev_hierarchy)\n",
    "            # Process article\n",
    "            article = self.article_processor.process_article(\n",
    "                article_id, articles_content[i], curr_hierarchy, \n",
    "                self.reference_graph, self.all_articles\n",
    "            )\n",
    "            self.articles_list.append(article)\n",
    "            print(\"-\"*50)\n",
    "\n",
    "            # Build hierarchy tree\n",
    "            node = self.hierarchy_tree\n",
    "            for lvl in self.parser.level_keys[:-1]:\n",
    "                if curr_hierarchy[lvl]:\n",
    "                    node = node[curr_hierarchy[lvl]]\n",
    "            node[\"articles\"] = node.get(\"articles\", []) + [article_id]\n",
    "\n",
    "            prev_hierarchy = curr_hierarchy.copy()\n",
    "\n",
    "        # Populate referenced_by field\n",
    "        for article in self.articles_list:\n",
    "            article[\"referenced_by\"] = self.reference_graph.get(article[\"article_id\"], [])\n",
    "\n",
    "        # Save output\n",
    "        output = {\n",
    "            \"articles\": self.articles_list,\n",
    "            \"hierarchy_tree\": dict(self.hierarchy_tree)\n",
    "        }\n",
    "        with open(self.json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(output, f, ensure_ascii=False, indent=2)\n",
    "        \n",
    "        print(\"Data saved to\", self.json_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "0806a15a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['partie', 'livre', 'titre', 'chapitre', 'section', 'sous_section']"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HierarchyParser().level_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "5795f689",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of articles: 2356\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing articles: 2it [00:00, 165.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article L100-1\n",
      "--------------------------------------------------\n",
      "Article L111-1\n",
      "--------------------------------------------------\n",
      "Data saved to code_assurances.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "processor = CodeAssurancesProcessor()\n",
    "processor.process()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mcp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
